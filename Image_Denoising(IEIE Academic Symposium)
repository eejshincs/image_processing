from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torch.nn as nn
import numpy as np
import random
import torch.utils.data as data
from os.path import join
from os import listdir
from PIL import Image
from torchvision.transforms import *
import torchvision.transforms.functional as TF
import torch.nn.functional as F


def is_image_file(filename):
    return any(filename.endswith(extension) for extension in [".png", ".jpg", ".jpeg",".bmp"])

def load_img(filepath, ch):
    if ch == 1:
      img = Image.open(filepath)#.convert('RGB')
    elif ch == 3:
      img = Image.open(filepath).convert('RGB')
    return img

class DatasetFromFolder(data.Dataset):
    def __init__(self, data_dir, ch):
        super(DatasetFromFolder, self).__init__()
        self.image_filenames_blur = [join(data_dir+'Train/Real_cropped/', data) for data in sorted(listdir(data_dir+'Train/Real_cropped/')) if is_image_file(data)]
        self.image_filenames_sharp = [join(data_dir+'Train/mean_cropped/', data) for data in sorted(listdir(data_dir+'Train/mean_cropped/')) if is_image_file(data)]
        self.ch = ch

    def __getitem__(self, index):
        img_blur = load_img(self.image_filenames_blur[index], self.ch)
        img_sharp = load_img(self.image_filenames_sharp[index], self.ch)
        img_blur = transforms.ToTensor()(img_blur)
        img_sharp = transforms.ToTensor()(img_sharp)


        return img_blur, img_sharp

    def __len__(self):
        return len(self.image_filenames_blur)

class DatasetFromFolderEval(data.Dataset):
    def __init__(self, data_dir, ch):
        super(DatasetFromFolderEval, self).__init__()
        self.image_filenames_blur = [join(data_dir+'Test/Real_cropped/', data) for data in sorted(listdir(data_dir+'Test/Real_cropped/')) if is_image_file(data)]
        self.image_filenames_sharp = [join(data_dir+'Test/mean_cropped/', data) for data in sorted(listdir(data_dir+'Test/mean_cropped')) if is_image_file(data)]
        self.ch = ch

    def __getitem__(self, index):

        img_blur = load_img(self.image_filenames_blur[index], self.ch)
        img_sharp = load_img(self.image_filenames_sharp[index], self.ch)
        img_blur = transforms.ToTensor()(img_blur)
        img_sharp = transforms.ToTensor()(img_sharp)

        return img_blur, img_sharp

    def __len__(self):
        return len(self.image_filenames_blur)

def get_training_set(data_dir, ch):
    return DatasetFromFolder(data_dir, ch)

def get_eval_set(data_dir, ch):
    return DatasetFromFolderEval(data_dir, ch)


import torch
import torch.nn as nn
import torch.nn.init as init

class Higher(nn.Module):
    def __init__(self):
        super(Higher, self).__init__()
        padding = 1
        layers = []

        layers.append(nn.Conv2d(in_channels=3, out_channels=64, kernel_size=1, padding=0, bias=False))

        for _ in range(10):
            layers.append(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=padding, bias=False))
            layers.append(nn.GroupNorm(num_groups=16, num_channels=64)) 
            layers.append(nn.ReLU())
        layers.append(nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3, padding=padding, bias=False))
        self.cnn1 = nn.Sequential(*layers)
        self._initialize_weights()

    def forward(self, x):
        out = self.cnn1(x)
        return out

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                init.orthogonal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.GroupNorm): 
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)





class Lower(nn.Module):
    def __init__(self):
        super(Lower, self).__init__()
        padding = 1
        layers = []

        layers.append(nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=padding, bias=False))
        layers.append(nn.GroupNorm(num_groups=8, num_channels=16))
        layers.append(nn.ReLU())

        for _ in range(5):
            layers.append(nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=2, bias=False, dilation=2))
            layers.append(nn.ReLU())

        layers.append(nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=padding, bias=False))
        layers.append(nn.GroupNorm(num_groups=16, num_channels=32))
        layers.append(nn.ReLU())

        for _ in range(5):
            layers.append(nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=2, bias=False, dilation=2))
            layers.append(nn.ReLU())

        layers.append(nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=padding, bias=False))
        layers.append(nn.GroupNorm(num_groups=8, num_channels=16))
        layers.append(nn.ReLU())

        layers.append(nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, padding=padding, bias=False))

        self.cnn2 = nn.Sequential(*layers)
        self._initialize_weights()

    def forward(self, x):
        out = self.cnn2(x)
        return out

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                init.orthogonal_(m.weight)
                if m.bias is not None:
                    init.constant_(m.bias, 0)
            elif isinstance(m, nn.GroupNorm):
                init.constant_(m.weight, 1)
                init.constant_(m.bias, 0)

class Total_NN(nn.Module):
    def __init__(self, Higher, Lower):
        super(Total_NN, self).__init__()

        self.Higher = Higher()
        self.Lower = Lower()
        self.conv_tot = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=3, padding=1, bias=False)

    def forward(self, x):
        x1 = self.Higher(x)
        x2 = self.Lower(x)
        y = torch.cat((x1, x2), dim=1)
        result = self.conv_tot(y)+x
        return result


import torch._dynamo
torch._dynamo.config.suppress_errors = True


from __future__ import print_function
import argparse
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.backends.cudnn as cudnn
from torch.autograd import Variable
from torch.utils.data import DataLoader
import time

parser = argparse.ArgumentParser(description='Total_NN for Denoising')
parser.add_argument('--batch_size', type=int, default=35, help='training batch size')

parser.add_argument('--lr', type=float, default=1e-4, help='initial learning rate')
parser.add_argument('--start_iter', type=int, default=1, help='the starting epoch count')
parser.add_argument('--nEpochs', type=int, default=150, help='# of iter at starting learning rate')

parser.add_argument('--snapshots', type=int, default=10, help='save weight file cycle')

parser.add_argument('--pretrained', type=bool, default=False, help='Is pretrained')
parser.add_argument('--gpu_mode', type=bool, default=True, help='GPU mode')
parser.add_argument('--threads', type=int, default=1, help='threads')
parser.add_argument('--seed', type=int, default=123, help='random seed to use')
parser.add_argument('--gpus', type=int, default=1, help='GPU nums')

parser.add_argument('--model_type', default='Total_NN', help='model name')
parser.add_argument('--image_channels', type=int, default=3, help='number of channels')
parser.add_argument('--kernel_size', type=int, default=3, help='kernel size')

parser.add_argument('--data_dir', type=str, default='/content/drive/MyDrive/Colab Notebooks AI/Dataset/', help='dataset directory') 
parser.add_argument('--save_folder', type=str, default='/content/drive/MyDrive/Colab Notebooks AI/denoise_result10/', help='weight file save location') 
args = parser.parse_args('')

gpus_list = range(args.gpus)
cudnn.benchmark = True
print(args)

def train(epoch):
    epoch_loss = 0
    model.train()
    for iteration, batch in enumerate(training_data_loader):
        img_blur, img_sharp = Variable(batch[0]), Variable(batch[1])
        #print(img_blur.shape)
        #print(img_sharp.shape)

        if cuda:
            img_sharp = img_sharp.cuda(gpus_list[0])
            img_blur = img_blur.cuda(gpus_list[0])

        optimizer.zero_grad()  
        t0 = time.time()

        prediction = model(img_blur)
        #print(img_sharp.shape)
        #print(prediction.shape)
        loss = criterion(prediction, img_sharp)

        t1 = time.time()
        epoch_loss += loss.data
        loss.backward()
        optimizer.step()

        print("===> Epoch[{}]({}/{}): Loss: {:.4f} || Timer: {:.4f} sec.".format(epoch, iteration, len(training_data_loader), loss.data, (t1 - t0)))

    print("===> Epoch {} Complete: Avg. Loss: {:.4f}".format(epoch, epoch_loss / len(training_data_loader)))

def print_network(net):
    num_params = 0
    for param in net.parameters():
        num_params += param.numel()
    print(net)
    print('Total number of parameters: %d' % num_params)

def checkpoint(epoch):
    model_out_path = args.save_folder+"epoch_{}.pth".format(epoch)
    torch.save(model.state_dict(), model_out_path)
    print("Checkpoint saved to {}".format(model_out_path))

if __name__ == '__main__':

    if not os.path.exists(args.save_folder):
        os.makedirs(args.save_folder)

    cuda = args.gpu_mode
    if cuda and not torch.cuda.is_available():
        raise Exception("No GPU found, please run without --cuda")

    torch.manual_seed(args.seed)
    if cuda:
        torch.cuda.manual_seed(args.seed)

    print('===> Loading datasets')           
    train_set = get_training_set(args.data_dir, args.image_channels)
    training_data_loader = DataLoader(dataset=train_set, num_workers=args.threads, batch_size=args.batch_size, shuffle=True)

    print('===> Building model ', args.model_type)
    if args.model_type == 'Total_NN':
        model = Total_NN(Higher, Lower)
        model = torch.nn.DataParallel(model, device_ids=gpus_list)

    criterion = nn.L1Loss()

    if args.pretrained:
        model_name = args.save_folder + "epoch_{}.pth".format(args.start_iter - 1)
        if os.path.exists(model_name):
            model.load_state_dict(torch.load(model_name, map_location=lambda storage, loc: storage))
            print('Pre-trained Denoising model is loaded.')

    if cuda:
        model = model.cuda(gpus_list[0])
        criterion = criterion.cuda(gpus_list[0])

    optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(0.9, 0.999), eps=1e-8)

    for epoch in range(args.start_iter, args.nEpochs + 1):
        train(epoch)

        if (epoch) % (args.nEpochs) == 0:
            for param_group in optimizer.param_groups:
                param_group['lr'] /= 10.0
            print('Learning rate decay: lr={}'.format(optimizer.param_groups[0]['lr']))

        if (epoch) % (args.snapshots) == 0:
            checkpoint(epoch)
